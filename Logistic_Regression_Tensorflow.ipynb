{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression using Tensorflow\n",
    "Applico la regressione logistica a un dataset semplice, ha una struttura tabellare. Sulla base di vari attributi di persone si cerca di predire se il reddito supero 50k dollari."
   ],
   "id": "3137a2e4dc07877b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:43:50.173075Z",
     "start_time": "2024-10-09T14:43:50.154148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data import get_adult\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import yaml\n",
    "import random\n",
    "import argparse"
   ],
   "id": "2a7e12316d6a5e9f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self,\n",
    "                 d,  # Dimension of the input vector\n",
    "                 lr,  # learning rate\n",
    "                 momentum,  # momentum\n",
    "                 nesterov,  # NAG (if True) or classic momentum (False)\n",
    "                 batch_size,  # Batch size\n",
    "                 num_epochs,  # number of cycles through the whole training set\n",
    "                 seed,  # !=0 to reproduce from run to run\n",
    "                 ):\n",
    "        # lock the seed for reproducibility\n",
    "        tf.random.set_seed(seed)\n",
    "        self.d = d\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        # Declare parameters as tensorflow variables\n",
    "        self.w = tf.Variable(tf.random.normal(shape=[d, 1]), name='w')\n",
    "        self.b = tf.Variable(tf.random.normal(shape=[1, 1]), name='b')\n",
    "        self.optimizer = tf.optimizers.SGD(\n",
    "            learning_rate=lr,\n",
    "            momentum=momentum,\n",
    "            nesterov=nesterov)\n",
    "\n",
    "    def _compute_logits(self, X):\n",
    "        f = tf.matmul(X, self.w) + self.b \n",
    "        return f\n",
    "    \n",
    "    def _step(self, X, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            f = self._compute_logits(X)\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=f, labels=y) # calcolo della loss passando i logits e labels\n",
    "            loss = tf.reduce_mean(loss) # media su tutti gli elementi del mini batch\n",
    "        gradients = tape.gradient(loss, [self.w, self.b]) # recupero il gradiente\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.w, self.b])) # applico gradiente\n",
    "        prediction = tf.round(tf.sigmoid(f)) # 1 se > 0.5, altrimenti 0\n",
    "        correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32) \n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        return loss,accuracy\n",
    "    \n",
    "    def _metrics(self, X, y): # serve per vedere l'andamento dell'ottimizzazione con Tensorboard\n",
    "        f = self._compute_logits(X)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=f, labels=y)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        prediction = tf.round(tf.sigmoid(f))\n",
    "        correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        return loss,accuracy\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_test, y_test): \n",
    "        n,d = X_train.shape\n",
    "        assert d==self.d # controllo che le dimensioni siano corrette a runtime\n",
    "        random.seed(1234)\n",
    "        train_summary_writer = tf.summary.create_file_writer('tensorboard/log_regr/train')\n",
    "        test_summary_writer = tf.summary.create_file_writer('tensorboard/log_regr/test')\n",
    "        for epoch in range(self.num_epochs): \n",
    "            idx=list(range(n)) # lista di indici, n = dimensione training set\n",
    "            random.shuffle(idx) # mescolo la lista con metodo random shuffle (visto a lezione)\n",
    "            losses,accs=[],[] # per salvare loss e accuracy dei mini batch\n",
    "            num_batches = n//self.batch_size \n",
    "            for b in range(num_batches):\n",
    "                mb_idx=np.array(idx[b*self.batch_size:(b+1)*self.batch_size]) # prendo un sottoinsieme di indici per il mini batch\n",
    "                X_mb=X_train[mb_idx] # estraggo input\n",
    "                y_mb=np.matrix(y_train[mb_idx]).T # estraggo label\n",
    "                loss,acc = self._step(X_mb,y_mb) # eseguo uno step di ottimizzazione\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            print(f\"Epoch: {epoch+1:4d}\", end=\" \")\n",
    "            print(f\"Loss: {np.array(losses).mean():.5f}\", end=\" \")\n",
    "            print(f\"Acc: {100*np.array(accs).mean():.2f}%\")\n",
    "            with train_summary_writer.as_default(): \n",
    "                loss,acc = self._metrics(X_train,np.matrix(y_train).T)\n",
    "                tf.summary.scalar('loss', loss, step=epoch)\n",
    "                tf.summary.scalar('accuracy',acc, step=epoch)\n",
    "            with test_summary_writer.as_default():\n",
    "                loss,acc = self._metrics(X_test,np.matrix(y_test).T)\n",
    "                tf.summary.scalar('loss', loss, step=epoch)\n",
    "                tf.summary.scalar('accuracy',acc, step=epoch)\n",
    "                \n",
    "    def predict(self, X_np): # fa inferenza sul test set calcolando i logits\n",
    "        n,d = X_np.shape\n",
    "        assert d == self.d\n",
    "        pred=self._compute_logits(X_np)\n",
    "        return (pred>0).numpy().flatten()\n",
    "    \n",
    "\n",
    "def main(opts):\n",
    "    X_train, y_train, X_test, y_test = get_adult(opts.datafile) # carico il dataset\n",
    "    \n",
    "    # costruisco il modello recuperando parametri da opts\n",
    "    clf = LogisticRegression(\n",
    "        X_train.shape[1],\n",
    "        lr=opts.lr,\n",
    "        momentum=opts.momentum,\n",
    "        nesterov=opts.nesterov,\n",
    "        batch_size=opts.batch_size,\n",
    "        num_epochs=opts.num_epochs,\n",
    "        seed=opts.seed)\n",
    "    \n",
    "    clf.fit(X_train,y_train,X_test,y_test)\n",
    "    pred=clf.predict(X_test)\n",
    "    print(f'Test set accuracy: {np.mean(pred==y_test)*100:.2f}%')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config\", help='YAML Configuration file')\n",
    "    opts = yaml.load(open(parser.parse_args().config), Loader=yaml.Loader)\n",
    "    opts = SimpleNamespace(**opts)\n",
    "    with launch_ipdb_on_exception():\n",
    "        main(opts)"
   ],
   "id": "a765c215f7618839"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d4cb2a98561b2d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
